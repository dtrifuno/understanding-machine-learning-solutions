\chapter{A Formal Learning Model}

\begin{ex}
  Suppose that $\H$ is a PAC learnable hypothesis class for a binary
  classification task with a sample complexity given by $m_\H(\cdot,\cdot)$.

  Fix a $\delta\in (0,1)$ and
  $0<\epsilon_1\leq\epsilon_2<1$. Then, for every distribution $\D$, if the
  realizable hypothesis holds with respect to the labelling function $f$,
  running the learning algorithm on $m\geq m_\H(\epsilon_1, \delta)$ i.i.d.\
  examples generated by $\D$ and labeled by $f$, produces an $h$
  such that with probability of at least $1-\delta$,
  $L_{(\D,f)}(h)\leq \epsilon_1$. However, since
  $\epsilon_1\leq \epsilon_2$, it follows that, under the same assumptions,
  $L_{(\D,f)}(h)\leq \epsilon_2$ as well. Hence,
  $m\geq m_\H(\epsilon_1, \delta)$
  implies that $m\geq m_\H(\epsilon_2, \delta)$, i.e.\
  $m_\H(\epsilon_1, \delta)\geq m_\H(\epsilon_2, \delta)$.

  Next, fix an $\epsilon\in (0,1)$ and $0<\delta_1\leq\delta_2<1$. As before,
  for every distribution $\D$, if the realizable hypothesis holds with respect
  to the labelling function $f$, running the learning algorithm on
  $m\geq m_\H(\epsilon,\delta_1)$ i.i.d.\ examples generated by $\D$ and labeled
  by $f$ returns a hypothesis $h$ such that with probability at least
  $1-\delta_1$, $L_{(\D,f)}(h)\leq \epsilon$. Since $\delta_1\leq \delta_2$,
  this also holds with probability of at least $1-\delta_2$, and hence
  $m\geq m_\H(\epsilon, \delta_2)$ as well. Thus,
  $m_\H(\epsilon, \delta_1)\geq m_\H(\epsilon, \delta_2)$.
\end{ex}

\begin{ex}
  \begin{enumerate}
    \item[]
    \item Given a training set $\{(z_1, y_1), \ldots, (z_m, y_m)\}$,
          return $h_{z_i}$ for the first $(z_i, y_i)$ such that $y_i=1$.
          If there is no such example, return $h^-$.
    \item Note that if $f=h^-$, our algorithm will identify the correct
          hypothesis and have zero error. Hence, we may assume without loss of
          generality that $f=h_{z_0}$ for some $z_0\in\X$. In this case, our
          algorithm will have nonzero error if and only if the training set $S$
          does not contain $z_0$, which will occur with probability
          \[
            \D^m(\{S \mid z_0\not\in S\})
            =(1-\D(\{z_0\}))^m,
          \]
          in which case our algorithm will erroneously
          return $h^-$ with error
          \[
            L_{(\D, f)}(h^-)=\D(\{z \mid h^-(z) \neq f(z) \})=\D(\{z_0\}).
          \]

          Thus, assuming that $L_{(\D, f)}(h^-)>\epsilon$ implies that
          $\D(\{z_0\})>\epsilon$, and hence
          \[
            (1-\D(\{z_0\}))^m<(1-\epsilon)^m\leq e^{-m\epsilon}.
          \]

          Thus, $\H_\text{Singleton}$ is PAC learnable, and to satisfy the
          bounds it suffices to find an $m$ such that
          $e^{-m\epsilon}\leq\delta$, i.e.\
          \[
            m_{\H_\text{Singleton}}(\epsilon, \delta)
            \geq \left\lceil\frac{\log(1/\delta)}{\epsilon}\right\rceil.
          \]

  \end{enumerate}
\end{ex}

\begin{ex}
  Given a training set $S=\{(\x_i, y_i)\}$, we can let
  \[
    \hat{r}=\max_{\substack{(\x_i, y_i)\in S \\ y_i = 1}} |\x_i|
  \]
  and return $h_{\hat{r}}$ as our hypothesis.

  Fix $\epsilon, \delta\in (0,1)$. Suppose that the true labelling function is
  given by $h_R$ for some $R\in \R_+$, and consider the annulus $A_\epsilon$
  centered at the origin, with large radius $R$ and small radius chosen such
  that $\D(A_\epsilon)=\epsilon$. Note that then
  \[
    L_{(\D, h_R)}(h_{\hat{r}})\geq\D(A_\epsilon) =\epsilon
  \]
  if and only if $S\cap A_\epsilon=\emptyset$. However, this can only occur with
  probability
  \[
    (1-\D(A_\epsilon))^m=(1-\epsilon)^m\leq e^{-m\epsilon}.
  \]
  Hence, $\H$ is PAC learnable, and it suffices to choose an $m$ such
  that $e^{-m\epsilon}\leq \delta$, i.e.
  \[
    m_{\H}(\epsilon,\delta)
    \geq \left\lceil\frac{\log(1/\delta)}{\epsilon}\right\rceil.
  \]
\end{ex}

\begin{ex}
  We begin by arguing that the hypothesis class is PAC learnable, by noting that
  it is finite, since each variable can either appear as-is, appear as its
  negation or not appear at all in a particular conjunction. Therefore,
  accounting for the additional always negative hypothesis, it follows that
  $|\H|=3^d+1$, and therefore, by Corollary 3.2, our hypothesis class is PAC
  learnable with sample complexity
  \[
    m_\H(\epsilon,\delta)\leq
    \left\lceil\frac{\log((3^d+1)/\delta)}{\epsilon}\right\rceil
    =\left\lceil\frac{d\log{3}+\log(1/\delta)}{\epsilon}\right\rceil.
  \]

  Next, we will give an algorithm $A$ that implements the ERM rule for learning
  this hypothesis class. Given a training set $S=\{(\vec{s}_i, y_i)\}_{i=1}^m$,
  start with the candidate hypothesis
  $\bigwedge_{i=1}^d x_i\wedge \bar{x}_i$ and then for each positively labeled
  example $\vec{s}$, modify the candidate hypothesis by, for each $i\in[d]$,
  removing the $x_i$ term if $s_i=0$ or the $\bar{x}_i$ term if $s_i=1$.
  Note that this makes the new candidate hypothesis label $\vec{s}$ as
  $1$, and that since we are only removing terms, all previously seen examples
  that were labeled as $1$, will also be labeled $1$ by the new
  candidate hypothesis. Moreover, it is clear that this produces the most
  conservative hypothesis that labels all positive examples as $1$, in the sense
  that if given another $h\in \H$ such that $h(\vec{s})=1$ for all
  $(\vec{s},1)\in S$, it follows that if $h(\x)=0$ for some $\x\in\X$, then
  $A(S)(\x)=0$ as well. Therefore, assuming realizability, the hypothesis $A(S)$
  cannot mislabel any examples labeled $0$ and hence will have zero empirical
  loss over $S$.

  Since the algorithm only needs to see each of the $m$ examples once, and can
  check and modify the candidate hypothesis independently and in constant time
  for each of the $d$ coordinates of a particular example, it follows that the
  runtime of this algorithm is $O(md)$.
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}
