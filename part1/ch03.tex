\chapter{A Formal Learning Model}

\begin{ex}
  Suppose that $\H$ is a PAC learnable hypothesis class for a binary
  classification task with a sample complexity given by $m_\H(\cdot,\cdot)$.

  Fix a $\delta\in (0,1)$ and
  $0<\epsilon_1\leq\epsilon_2<1$. Then, for every distribution $\D$, if the
  realizable hypothesis holds with respect to the labelling function $f$,
  running the learning algorithm on $m\geq m_\H(\epsilon_1, \delta)$ i.i.d.\
  examples generated by $\D$ and labeled by $f$, produces an $h$
  such that with probability of at least $1-\delta$,
  $L_{(\D,f)}(h)\leq \epsilon_1$. However, since
  $\epsilon_1\leq \epsilon_2$, it follows that, under the same assumptions,
  $L_{(\D,f)}(h)\leq \epsilon_2$ as well. Hence,
  $m\geq m_\H(\epsilon_1, \delta)$
  implies that $m\geq m_\H(\epsilon_2, \delta)$, i.e.\
  $m_\H(\epsilon_1, \delta)\geq m_\H(\epsilon_2, \delta)$.

  Next, fix an $\epsilon\in (0,1)$ and $0<\delta_1\leq\delta_2<1$. As before,
  for every distribution $\D$, if the realizable hypothesis holds with respect
  to the labelling function $f$, running the learning algorithm on
  $m\geq m_\H(\epsilon,\delta_1)$ i.i.d.\ examples generated by $\D$ and labeled
  by $f$ returns a hypothesis $h$ such that with probability at least
  $1-\delta_1$, $L_{(\D,f)}(h)\leq \epsilon$. Since $\delta_1\leq \delta_2$,
  this also holds with probability of at least $1-\delta_2$, and hence
  $m\geq m_\H(\epsilon, \delta_2)$ as well. Thus,
  $m_\H(\epsilon, \delta_1)\geq m_\H(\epsilon, \delta_2)$.
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}
