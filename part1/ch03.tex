\chapter{A Formal Learning Model}

\begin{ex}
  Suppose that $\H$ is a PAC learnable hypothesis class for a binary
  classification task with a sample complexity given by $m_\H(\cdot,\cdot)$.

  Fix a $\delta\in (0,1)$ and
  $0<\epsilon_1\leq\epsilon_2<1$. Then, for every distribution $\D$, if the
  realizable hypothesis holds with respect to the labelling function $f$,
  running the learning algorithm on $m\geq m_\H(\epsilon_1, \delta)$ i.i.d.\
  examples generated by $\D$ and labeled by $f$, produces an $h$
  such that with probability of at least $1-\delta$,
  $L_{(\D,f)}(h)\leq \epsilon_1$. However, since
  $\epsilon_1\leq \epsilon_2$, it follows that, under the same assumptions,
  $L_{(\D,f)}(h)\leq \epsilon_2$ as well. Hence,
  $m\geq m_\H(\epsilon_1, \delta)$
  implies that $m\geq m_\H(\epsilon_2, \delta)$, i.e.\
  $m_\H(\epsilon_1, \delta)\geq m_\H(\epsilon_2, \delta)$.

  Next, fix an $\epsilon\in (0,1)$ and $0<\delta_1\leq\delta_2<1$. As before,
  for every distribution $\D$, if the realizable hypothesis holds with respect
  to the labelling function $f$, running the learning algorithm on
  $m\geq m_\H(\epsilon,\delta_1)$ i.i.d.\ examples generated by $\D$ and labeled
  by $f$ returns a hypothesis $h$ such that with probability at least
  $1-\delta_1$, $L_{(\D,f)}(h)\leq \epsilon$. Since $\delta_1\leq \delta_2$,
  this also holds with probability of at least $1-\delta_2$, and hence
  $m\geq m_\H(\epsilon, \delta_2)$ as well. Thus,
  $m_\H(\epsilon, \delta_1)\geq m_\H(\epsilon, \delta_2)$.
\end{ex}

\begin{ex}
  \begin{enumerate}
    \item[]
    \item Given a training set $\{(z_1, y_1), \ldots, (z_m, y_m)\}$,
          return $h_{z_i}$ for the first $(z_i, y_i)$ such that $y_i=1$.
          If there is no such sample, return $h^-$.
    \item Note that if $f=h^-$, our algorithm will identify the correct
          hypothesis and have zero error. Hence, we may assume without loss of
          generality that $f=h_{z_0}$ for some $z_0\in\X$. In this case, our
          algorithm will have nonzero error if and only if the training set $S$
          does not contain $z_0$, which will occur with probability
          \[
            \D^m(\{S \mid z_0\not\in S\})
            =(1-\D(\{z_0\}))^m,
          \]
          in which case our algorithm will erroneously
          return $h^-$ with error
          \[
            L_{(\D, f)}(h^-)=\D(\{z \mid h^-(z) \neq f(z) \})=\D(\{z_0\}).
          \]

          Thus, assuming that $L_{(\D, f)}(h^-)>\epsilon$ implies that
          $\D(\{z_0\})>\epsilon$, and hence
          \[
            (1-\D(\{z_0\}))^m<(1-\epsilon)^m\leq e^{-m\epsilon}.
          \]

          Thus, $\H_\text{Singleton}$ is PAC learnable, and to satisfy the
          bounds it suffices to find an $m$ such that
          $e^{-m\epsilon}\leq\delta$, i.e.\
          \[
            m_{\H_\text{Singleton}}(\epsilon, \delta)
            \geq \left\lceil\frac{\log(1/\delta)}{\epsilon}\right\rceil.
          \]

  \end{enumerate}
\end{ex}

\begin{ex}
  Given a training set $S=\{(\x_i, y_i)\}$, we can let
  \[
    \hat{r}=\max_{\substack{(\x_i, y_i)\in S \\ y_i = 1}} |\x_i|
  \]
  and return $h_{\hat{r}}$ as our hypothesis.

  Fix $\epsilon, \delta\in (0,1)$. Suppose that the true labelling function is
  given by $h_R$ for some $R\in \R_+$, and consider the annulus $A_\epsilon$
  centered at the origin, with large radius $R$ and small radius chosen such
  that $\D(A_\epsilon)=\epsilon$. Note that then
  \[
    L_{(\D, h_R)}(h_{\hat{r}})\geq\D(A_\epsilon) =\epsilon
  \]
  if and only if $S\cap A_\epsilon=\emptyset$. However, this can only occur with
  probability
  \[
    (1-\D(A_\epsilon))^m=(1-\epsilon)^m\leq e^{-m\epsilon}.
  \]
  Hence, $\H$ is PAC learnable, and it suffices to choose an $m$ such
  that $e^{-m\epsilon}\leq \delta$, i.e.
  \[
    m_{\H}(\epsilon,\delta)
    \geq \left\lceil\frac{\log(1/\delta)}{\epsilon}\right\rceil.
  \]
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}
