\chapter{A Formal Learning Model}

\begin{ex}
  Suppose that $\H$ is a PAC learnable hypothesis class for a binary
  classification task with a sample complexity given by $m_\H(\cdot,\cdot)$.

  Fix a $\delta\in (0,1)$ and
  $0<\epsilon_1\leq\epsilon_2<1$. Then, for every distribution $\D$, if the
  realizable hypothesis holds with respect to the labelling function $f$,
  running the learning algorithm on $m\geq m_\H(\epsilon_1, \delta)$ i.i.d.\
  examples generated by $\D$ and labeled by $f$, produces an $h$
  such that with probability of at least $1-\delta$,
  $L_{(\D,f)}(h)\leq \epsilon_1$. However, since
  $\epsilon_1\leq \epsilon_2$, it follows that, under the same assumptions,
  $L_{(\D,f)}(h)\leq \epsilon_2$ as well. Hence,
  $m\geq m_\H(\epsilon_1, \delta)$
  implies that $m\geq m_\H(\epsilon_2, \delta)$, i.e.\
  $m_\H(\epsilon_1, \delta)\geq m_\H(\epsilon_2, \delta)$.

  Next, fix an $\epsilon\in (0,1)$ and $0<\delta_1\leq\delta_2<1$. As before,
  for every distribution $\D$, if the realizable hypothesis holds with respect
  to the labelling function $f$, running the learning algorithm on
  $m\geq m_\H(\epsilon,\delta_1)$ i.i.d.\ examples generated by $\D$ and labeled
  by $f$ returns a hypothesis $h$ such that with probability at least
  $1-\delta_1$, $L_{(\D,f)}(h)\leq \epsilon$. Since $\delta_1\leq \delta_2$,
  this also holds with probability of at least $1-\delta_2$, and hence
  $m\geq m_\H(\epsilon, \delta_2)$ as well. Thus,
  $m_\H(\epsilon, \delta_1)\geq m_\H(\epsilon, \delta_2)$.
\end{ex}

\begin{ex}
  \begin{enumerate}
    \item[]
    \item Given a training set $\{(z_1, y_1), \ldots, (z_m, y_m)\}$,
          return $h_{z_i}$ for the first $(z_i, y_i)$ such that $y_i=1$.
          If there is no such sample, return $h^-$.
    \item Note that if $f=h^-$, our algorithm will identify the correct
          hypothesis and have zero error. Hence, we may assume without loss of
          generality that $f=h_{z_0}$ for some $z_0\in\X$. In this case, our
          algorithm will have nonzero error if and only if the training set $S$
          does not contain $z_0$, which will occur with probability
          \[
            \D^m(\{S \mid z_0\not\in S\})
            =(1-\D(\{z_0\}))^m,
          \]
          in which case our algorithm will erroneously
          return $h^-$ with error
          \[
            L_{(\D, f)}(h^-)=\D(\{z \mid h^-(z) \neq f(z) \})=\D(\{z_0\}).
          \]

          Thus, assuming that $L_{(\D, f)}(h^-)>\epsilon$ implies that
          $\D(\{z_0\})>\epsilon$, and hence
          \[
            (1-\D(\{z_0\}))^m<(1-\epsilon)^m\leq e^{-m\epsilon}.
          \]

          Thus, it suffices to find an $m$ such that $e^{-m\epsilon} \leq\delta$,
          i.e.\
          \[
            m \geq \frac{\log(1/\delta)}{\epsilon}.
          \]

  \end{enumerate}
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}

\begin{ex}
\end{ex}
